import torch
import sentencepiece as spm
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelForCausalLM
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
from bert_score import score as bert_score
import pandas as pd
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
matplotlib.rcParams['font.size'] = 12

# æ£€æŸ¥ç³»ç»Ÿå­—ä½“
import platform
if platform.system() == 'Windows':
    matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
elif platform.system() == 'Darwin':  # macOS
    matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Heiti TC', 'Heiti SC']
else:  # Linux
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'WenQuanYi Micro Hei']

class ModelComparisonEvaluator:
    def __init__(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½tokenizer
        self.sp = spm.SentencePieceProcessor()
        self.sp.load("tokenizer.model")
        self.vocab_size = self.sp.get_piece_size()
        print(f"è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        
        # æ¨¡å‹å‚æ•°
        self.d_model = 512
        self.max_seq_len = 2048
        self.h = 8
        self.Nx = 6
        self.dropout_rate = 0.2
        
        # åŠ è½½æ‚¨çš„æ¨¡å‹
        self.your_model = self.load_your_model()
        
        # ã€ä¿®æ”¹ç‚¹ 1ã€‘: æ›¿æ¢ä¸ºä¸­æ–‡æ¨¡å‹åç§°ï¼Œä¿ç•™æ‚¨çš„æ¨¡å‹
        self.model_names = [
            "Your Model",
            "GPT2 (ä¸­æ–‡)",
            "Dialogue GPT2 (ä¸­æ–‡)"
        ]

        # æ–°çš„æ¨¡å‹ ID æ˜ å°„
        self.hf_model_ids = {
            "GPT2 (ä¸­æ–‡)": "uer/gpt2-chinese-cluecorpussmall",
            "Dialogue GPT2 (ä¸­æ–‡)": "IDEA-CCNL/Wenzhong-GPT2-110M" # <-- å·²ä¿®æ­£
        }

        # åŠ è½½å¤šä¸ªå¯¹æ¯”æ¨¡å‹
        self.models = {
            'your_model': ('Your Model', self.your_model, None),
            'gpt2_chinese': ('GPT2 (ä¸­æ–‡)', *self.load_gpt2_chinese_model()),
            'dialogue_gpt2': ('Dialogue GPT2 (ä¸­æ–‡)', *self.load_dialogue_gpt2_model())
        }
        
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        
        # æµ‹è¯•æç¤º
        self.test_prompts = [
            "å…³é”®è¯: ä¿¡ å¤©æ¶¯ æ™šé£",
            "å…³é”®è¯: é£ é›¾ å¯‚å¯", 
            "å…³é”®è¯: è´´å¿ƒ æ”¹å˜ è‡ªä¿¡",
            "å…³é”®è¯: åˆå¤œ å¯’å†¬ å¿ƒåŠ¨",
            "å…³é”®è¯: æ€è€ƒ æ¨ç† åˆ†æ",
            "å…³é”®è¯: æœˆå…‰ æ€å¿µ è¿œæ–¹",
            "å…³é”®è¯: æ¢¦æƒ³ åšæŒ æˆåŠŸ",
            "å…³é”®è¯: æ˜¥å¤© å¸Œæœ› æ–°ç”Ÿ",
            "å…³é”®è¯: å­¦ä¹  è¿›æ­¥ æˆé•¿",
            "å…³é”®è¯: å‹è°Š ä¿¡ä»» é™ªä¼´"
        ]
    
    def load_your_model(self):
        """åŠ è½½æ‚¨çš„GPTæ¨¡å‹"""
        from model_optimized import MemoryOptimizedBigramLM
        
        model = MemoryOptimizedBigramLM(
            vocab_size=self.vocab_size,
            d_model=self.d_model,
            max_seq_len=self.max_seq_len,
            h=self.h,
            Nx=self.Nx,
            dropout_rate=self.dropout_rate
        )
        
        try:
            checkpoint = torch.load("saved_models/gpt_model_final_20251003_124248.pth", 
                                  map_location=self.device, weights_only=False)
            state_dict = checkpoint['model_state_dict']
            filtered_state_dict = {k: v for k, v in state_dict.items() if 'mask' not in k}
            model.load_state_dict(filtered_state_dict, strict=False)
            print("âœ… æˆåŠŸåŠ è½½æ‚¨çš„GPTæ¨¡å‹")
        except Exception as e:
            print(f"âŒ åŠ è½½æ‚¨çš„æ¨¡å‹å¤±è´¥: {e}")
            return None
        
        model = model.to(self.device)
        model.eval()
        return model
    
    def load_distilgpt2_model(self):
        """åŠ è½½DistilGPT2æ¨¡å‹"""
        try:
            model_name = "distilgpt2"  # 82Må‚æ•°ï¼Œ6å±‚ï¼Œ768 hidden
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(model_name)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = model.to(self.device)
            model.eval()
            print("âœ… æˆåŠŸåŠ è½½DistilGPT2æ¨¡å‹ (82Må‚æ•°)")
            return model, tokenizer
        except Exception as e:
            print(f"âŒ åŠ è½½DistilGPT2æ¨¡å‹å¤±è´¥: {e}")
            return None, None
    
    def load_gpt2_model(self):
        """åŠ è½½æ ‡å‡†GPT2æ¨¡å‹"""
        try:
            model_name = "gpt2"  # 124Må‚æ•°
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(model_name)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = model.to(self.device)
            model.eval()
            print("âœ… æˆåŠŸåŠ è½½GPT2æ¨¡å‹ (124Må‚æ•°)")
            return model, tokenizer
        except Exception as e:
            print(f"âŒ åŠ è½½GPT2æ¨¡å‹å¤±è´¥: {e}")
            return None, None
    
    def load_tinystories_model(self):
        """åŠ è½½TinyStoriesæ¨¡å‹"""
        try:
            # ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ä½œä¸ºTinyStoriesçš„æ›¿ä»£
            model_name = "microsoft/DialoGPT-small"  # çº¦117Må‚æ•°ï¼Œä½œä¸ºå°æ¨¡å‹å¯¹æ¯”
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(model_name)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = model.to(self.device)
            model.eval()
            print("âœ… æˆåŠŸåŠ è½½TinyStoriesæ›¿ä»£æ¨¡å‹ (117Må‚æ•°)")
            return model, tokenizer
        except Exception as e:
            print(f"âŒ åŠ è½½TinyStoriesæ¨¡å‹å¤±è´¥: {e}")
            return None, None
    
    def load_gpt2_chinese_model(self):
        """åŠ è½½ä¸­æ–‡GPT2æ¨¡å‹"""
        try:
            model_name = "uer/gpt2-chinese-cluecorpussmall"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(model_name)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = model.to(self.device)
            model.eval()
            print("âœ… æˆåŠŸåŠ è½½ä¸­æ–‡GPT2æ¨¡å‹")
            return model, tokenizer
        except Exception as e:
            print(f"âŒ åŠ è½½ä¸­æ–‡GPT2æ¨¡å‹å¤±è´¥: {e}")
            return None, None
    
    def load_dialogue_gpt2_model(self):
        """åŠ è½½å¯¹è¯GPT2æ¨¡å‹"""
        try:
            model_name = "IDEA-CCNL/Wenzhong-GPT2-110M"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(model_name)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = model.to(self.device)
            model.eval()
            print("âœ… æˆåŠŸåŠ è½½å¯¹è¯GPT2æ¨¡å‹")
            return model, tokenizer
        except Exception as e:
            print(f"âŒ åŠ è½½å¯¹è¯GPT2æ¨¡å‹å¤±è´¥: {e}")
            return None, None
    
    def generate_with_your_model(self, prompt, max_new_tokens=200):
        """ä½¿ç”¨æ‚¨çš„æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
        if self.your_model is None:
            return ""
        
        temperature = 0.8
        top_k = 50
        repetition_penalty = 1.3
        
        prompt_tokens = self.sp.encode(prompt, out_type=int)
        context = torch.tensor([prompt_tokens], dtype=torch.long, device=self.device)
        
        with torch.no_grad():
            generated_tokens = self.your_model.generate(
                context, 
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_k=top_k,
                repetition_penalty=repetition_penalty
            )[0].tolist()
            
            generated_text = self.sp.decode(generated_tokens)
            response_start = generated_text.find(prompt) + len(prompt)
            response = generated_text[response_start:].strip()
            
            return response
    
    def generate_with_model(self, model_name, prompt, max_new_tokens=200):
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
        model_info = self.models.get(model_name)
        if not model_info or model_info[1] is None:
            return ""
        
        display_name, model, tokenizer = model_info
        
        if model_name == 'your_model':
            # ä½¿ç”¨æ‚¨çš„æ¨¡å‹
            return self.generate_with_your_model(prompt, max_new_tokens)
        else:
            # ä½¿ç”¨å…¶ä»–æ¨¡å‹
            poetry_prompt = f"è¯·æ ¹æ®ä»¥ä¸‹å…³é”®è¯åˆ›ä½œä¸€é¦–ä¼˜ç¾çš„è¯—æ­Œï¼š{prompt}\nè¯—æ­Œï¼š"
            
            inputs = tokenizer.encode(poetry_prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.3
                )
                
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                response_start = generated_text.find(poetry_prompt) + len(poetry_prompt)
                response = generated_text[response_start:].strip()
                
                # æ¸…ç†ä¹±ç ï¼šç§»é™¤éä¸­æ–‡å­—ç¬¦å’Œç‰¹æ®Šç¬¦å·
                import re
                cleaned_response = re.sub(r'[^\u4e00-\u9fff\u3000-\u303f\uff00-\uffefï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€\n\r]', '', response)
                cleaned_response = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€]{2,}', 'ï¼Œ', cleaned_response)
                cleaned_response = re.sub(r'\s+', ' ', cleaned_response).strip()
                
                if not cleaned_response:
                    return response
                
                return cleaned_response
    
    def calculate_bleu_score(self, generated, reference=None):
        """è®¡ç®—BLEUåˆ†æ•°"""
        if reference is None:
            # å¦‚æœæ²¡æœ‰å‚è€ƒæ–‡æœ¬ï¼Œä½¿ç”¨promptä½œä¸ºå‚è€ƒ
            reference = [generated.split()[:5]]  # ä½¿ç”¨å‰å‡ ä¸ªè¯ä½œä¸ºå‚è€ƒ
        
        smoothie = SmoothingFunction().method4
        try:
            score = sentence_bleu([reference], generated.split(), smoothing_function=smoothie)
            return score
        except:
            return 0.0
    
    def calculate_rouge_l(self, generated, reference=None):
        """è®¡ç®—ROUGE-Låˆ†æ•°"""
        if reference is None:
            reference = generated[:50]  # ä½¿ç”¨ç”Ÿæˆæ–‡æœ¬çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºå‚è€ƒ
        
        scores = self.rouge_scorer.score(reference, generated)
        return scores['rougeL'].fmeasure
    
    def calculate_bertscore(self, generated, reference=None):
        """è®¡ç®—BERTScore"""
        if reference is None:
            reference = generated  # è‡ªå‚è€ƒ
        
        try:
            P, R, F1 = bert_score([generated], [reference], lang="zh", verbose=False)
            return F1.item()
        except:
            return 0.0
    
    def calculate_distinct_n(self, text, n):
        """è®¡ç®—distinct-næŒ‡æ ‡"""
        words = text.split()
        if len(words) < n:
            return 0.0
        
        ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]
        unique_ngrams = len(set(ngrams))
        total_ngrams = len(ngrams)
        
        return unique_ngrams / total_ngrams if total_ngrams > 0 else 0.0
    
    def calculate_repetition_rate(self, text):
        """è®¡ç®—é‡å¤ç‡"""
        words = text.split()
        if len(words) < 2:
            return 0.0
        
        repeated_count = 0
        total_pairs = len(words) - 1
        
        for i in range(total_pairs):
            if words[i] == words[i+1]:
                repeated_count += 1
        
        return repeated_count / total_pairs if total_pairs > 0 else 0.0
    
    def calculate_coherence_score(self, text):
        """è®¡ç®—è¿è´¯æ€§åˆ†æ•°ï¼ˆåŸºäºå¥å­é•¿åº¦å’Œç»“æ„ï¼‰"""
        sentences = text.split('ã€‚')
        if len(sentences) < 2:
            return 0.5
        
        # ç®€å•çš„è¿è´¯æ€§è¯„ä¼°ï¼šå¥å­é•¿åº¦å˜åŒ–å’Œå¤šæ ·æ€§
        sentence_lengths = [len(sent) for sent in sentences if len(sent) > 0]
        if len(sentence_lengths) < 2:
            return 0.5
        
        # å¥å­é•¿åº¦æ ‡å‡†å·®ï¼ˆé€‚ä¸­çš„å˜åŒ–æ›´å¥½ï¼‰
        length_std = np.std(sentence_lengths)
        coherence = 1.0 - min(length_std / 20, 1.0)  # æ ‡å‡†åŒ–
        
        return coherence
    
    def evaluate_single_prompt(self, prompt):
        """è¯„ä¼°å•ä¸ªæç¤ºçš„æ‰€æœ‰æ¨¡å‹è¾“å‡º"""
        metrics = {}
        
        for model_name, (display_name, model, tokenizer) in self.models.items():
            if model is None:
                continue
                
            output = self.generate_with_model(model_name, prompt)
            
            metrics[model_name] = {
                'display_name': display_name,
                'output': output,
                'bleu': self.calculate_bleu_score(output),
                'rouge_l': self.calculate_rouge_l(output),
                'bertscore': self.calculate_bertscore(output),
                'distinct_1': self.calculate_distinct_n(output, 1),
                'distinct_2': self.calculate_distinct_n(output, 2),
                'repetition_rate': self.calculate_repetition_rate(output),
                'coherence': self.calculate_coherence_score(output),
                'length': len(output)
            }
        
        return metrics
    
    def run_comparison(self):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”è¯„ä¼°"""
        print("å¼€å§‹æ¨¡å‹å¯¹æ¯”è¯„ä¼°...")
        print("=" * 80)
        
        all_results = []
        
        for i, prompt in enumerate(self.test_prompts, 1):
            print(f"\nè¿›åº¦: {i}/{len(self.test_prompts)}")
            result = self.evaluate_single_prompt(prompt)
            result['prompt'] = prompt
            all_results.append(result)
        
        return all_results
    
    def analyze_results(self, all_results):
        """åˆ†æå¹¶å¯è§†åŒ–ç»“æœ"""
        # æå–æ‰€æœ‰æ¨¡å‹çš„æ•°æ®
        model_scores = {}
        for model_name in self.models.keys():
            model_scores[model_name] = []
        
        for result in all_results:
            for model_name, metrics in result.items():
                if model_name != 'prompt' and model_name in model_scores:
                    model_scores[model_name].append(metrics)
        
        # åˆ›å»ºDataFrameç”¨äºåˆ†æ
        model_dfs = {}
        for model_name, scores in model_scores.items():
            if scores:
                model_dfs[model_name] = pd.DataFrame(scores)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        metrics = ['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 
                  'repetition_rate', 'coherence', 'length']
        
        avg_scores = {}
        for model_name, df in model_dfs.items():
            for metric in metrics:
                avg_scores[f'{model_name}_{metric}'] = df[metric].mean()
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "="*80)
        print("å¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*80)
        
        for metric in metrics:
            print(f"\n{metric.upper():<15}:")
            model_avgs = []
            for model_name in self.models.keys():
                if model_name in model_dfs:
                    avg = avg_scores[f'{model_name}_{metric}']
                    display_name = self.models[model_name][0]
                    model_avgs.append((display_name, avg))
                    print(f"  {display_name:<20}: {avg:.4f}")
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            if model_avgs:
                best_model = max(model_avgs, key=lambda x: x[1])
                print(f"  æœ€ä½³æ¨¡å‹: {best_model[0]} ({best_model[1]:.4f})")
        
        return model_dfs, avg_scores
    
    def create_visualizations(self, model_dfs, avg_scores):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ - å±•ç¤º3ä¸ªæ¨¡å‹çš„ç»“æœ"""
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig1 = plt.figure(figsize=(18, 6))
        fig1.suptitle('Model Performance Comparison Analysis(picture 1/2 )', fontsize=16, fontweight='bold')
        
        # å®šä¹‰æ¨¡å‹é¢œè‰²å’Œæ ‡ç­¾
        model_colors = {
            'your_model': 'skyblue',
            'gpt2_chinese': 'lightcoral',
            'dialogue_gpt2': 'gold'
        }
        
        model_labels = {
            'your_model': 'Your Model',
            'gpt2_chinese': 'GPT2 (ä¸­æ–‡)',
            'dialogue_gpt2': 'Dialogue GPT2 (ä¸­æ–‡)'
        }
        
        # ç¬¬ä¸€æ’ï¼š3ä¸ªä¸»è¦å¯¹æ¯”å›¾
        # ä¸»è¦æŒ‡æ ‡å¯¹æ¯” - æŸ±çŠ¶å›¾
        ax1 = fig1.add_subplot(1, 3, 1) # 1è¡Œ3åˆ—çš„ç¬¬1ä¸ª
        metrics_to_plot = ['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 'coherence']
        metric_names = ['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Distinct-2', 'Coherence']
        
        x = np.arange(len(metrics_to_plot))
        width = 0.2
        
        for i, model_name in enumerate(self.models.keys()):
            if model_name in model_dfs:
                model_avgs = [avg_scores[f'{model_name}_{metric}'] for metric in metrics_to_plot]
                ax1.bar(x + i*width - width*1.5, model_avgs, width, 
                       label=model_labels[model_name], alpha=0.8, color=model_colors[model_name])
        
        ax1.set_xlabel('Evaluation Metrics', fontsize=12)
        ax1.set_ylabel('Score', fontsize=12)
        ax1.set_title('Main Metrics Comparison', fontsize=14, fontweight='bold')
        ax1.set_xticks(x)
        ax1.set_xticklabels(metric_names, rotation=45, fontsize=10)
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # é‡å¤ç‡å¯¹æ¯” - ç®±çº¿å›¾
        ax2 = fig1.add_subplot(1, 3, 2) # 1è¡Œ3åˆ—çš„ç¬¬2ä¸ª
        repetition_data = []
        labels = []
        for model_name in self.models.keys():
            if model_name in model_dfs:
                repetition_data.append(model_dfs[model_name]['repetition_rate'])
                labels.append(model_labels[model_name])
        
        box_plot = ax2.boxplot(repetition_data, labels=labels, patch_artist=True)
        
        # è®¾ç½®é¢œè‰²
        for i, (patch, model_name) in enumerate(zip(box_plot['boxes'], self.models.keys())):
            if model_name in model_colors:
                patch.set_facecolor(model_colors[model_name])
        
        ax2.set_ylabel('Repetition Rate', fontsize=12)
        ax2.set_title('Repetition Rate Distribution', fontsize=14, fontweight='bold')
        ax2.tick_params(axis='x', rotation=45, labelsize=10)
        ax2.grid(True, alpha=0.3)
        
        # è¾“å‡ºé•¿åº¦å¯¹æ¯” - ç®±çº¿å›¾
        ax3 = fig1.add_subplot(1, 3, 3) # 1è¡Œ3åˆ—çš„ç¬¬3ä¸ª
        length_data = []
        labels = []
        for model_name in self.models.keys():
            if model_name in model_dfs:
                length_data.append(model_dfs[model_name]['length'])
                labels.append(model_labels[model_name])
        
        length_plot = ax3.boxplot(length_data, labels=labels, patch_artist=True)
        
        for i, (patch, model_name) in enumerate(zip(length_plot['boxes'], self.models.keys())):
            if model_name in model_colors:
                patch.set_facecolor(model_colors[model_name])
        
        ax3.set_ylabel('Output Length (characters)', fontsize=12)
        ax3.set_title('Output Length Comparison', fontsize=14, fontweight='bold')
        ax3.tick_params(axis='x', rotation=45, labelsize=10)
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig('model_comparison_results_1.png', dpi=300, bbox_inches='tight')


        fig2 = plt.figure(figsize=(18, 6))
        fig2.suptitle('Model Deep Analysis and Trends (picture 2/2)', fontsize=16, fontweight='bold')
        # ç¬¬äºŒæ’ï¼š3ä¸ªåˆ†æå›¾
        # æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾ - æ‚¨çš„æ¨¡å‹
        ax4 = fig2.add_subplot(1, 3, 1) # 1è¡Œ3åˆ—çš„ç¬¬1ä¸ª
        if 'your_model' in model_dfs:
            correlation_matrix_your = model_dfs['your_model'][['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 'coherence']].corr()
            sns.heatmap(correlation_matrix_your, annot=True, cmap='coolwarm', center=0, ax=ax4, 
                       xticklabels=['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Distinct-2', 'Coherence'],
                       yticklabels=['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Distinct-2', 'Coherence'],
                       annot_kws={"size": 9})
            ax4.set_title('Your Model: Metric Correlations', fontsize=14, fontweight='bold')
        
        # æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾ - ä¸­æ–‡GPT2æ¨¡å‹
        ax5 = fig2.add_subplot(1, 3, 2) # 1è¡Œ3åˆ—çš„ç¬¬1ä¸ª
        if 'gpt2_chinese' in model_dfs:
            correlation_matrix_gpt2 = model_dfs['gpt2_chinese'][['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 'coherence']].corr()
            sns.heatmap(correlation_matrix_gpt2, annot=True, cmap='coolwarm', center=0, ax=ax5,
                       xticklabels=['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Distinct-2', 'Coherence'],
                       yticklabels=['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Distinct-2', 'Coherence'],
                       annot_kws={"size": 9})
            ax5.set_title('GPT2 (ä¸­æ–‡): Metric Correlations', fontsize=14, fontweight='bold')
        
        # æ€§èƒ½é›·è¾¾å›¾ - æ‰€æœ‰æ¨¡å‹
        ax6 = fig2.add_subplot(1, 3, 3) # 1è¡Œ3åˆ—çš„ç¬¬1ä¸ª
        
        # é€‰æ‹©å‡ ä¸ªå…³é”®æŒ‡æ ‡
        radar_metrics = ['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'coherence']
        radar_names = ['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Coherence']
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2*np.pi, len(radar_metrics), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        for model_name in self.models.keys():
            if model_name in model_dfs:
                model_radar = [avg_scores[f'{model_name}_{metric}'] for metric in radar_metrics]
                
                # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
                max_vals = [max([avg_scores[f'{m}_{metric}'] for m in self.models.keys() if m in model_dfs]) 
                           for metric in radar_metrics]
                model_radar_norm = [model_radar[i] / max_vals[i] if max_vals[i] > 0 else 0 
                                  for i in range(len(radar_metrics))]
                model_radar_norm += model_radar_norm[:1]
                
                ax6.plot(angles, model_radar_norm, 'o-', linewidth=2, 
                        label=model_labels[model_name], color=model_colors[model_name])
                ax6.fill(angles, model_radar_norm, alpha=0.25, color=model_colors[model_name])
        
        ax6.set_xticks(angles[:-1])
        ax6.set_xticklabels(radar_names, fontsize=10)
        ax6.set_ylim(0, 1)
        ax6.set_title('Performance Radar Chart', fontsize=14, fontweight='bold')
        ax6.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
        ax6.grid(True, alpha=0.3)
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig('model_comparison_results_2.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return fig1,fig2

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„æ¨¡å‹å¯¹æ¯”è¯„ä¼°"""
    print("ğŸš€ å¼€å§‹å¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°")
    print("=" * 80)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelComparisonEvaluator()
    
    # è¿è¡Œå¯¹æ¯”è¯„ä¼°
    all_results = evaluator.run_comparison()
    
    # åˆ†æç»“æœ
    model_dfs, avg_scores = evaluator.analyze_results(all_results)
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    evaluator.create_visualizations(model_dfs, avg_scores)
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSV
    detailed_results = []
    for result in all_results:
        row = {'prompt': result['prompt']}
        for model_name, metrics in result.items():
            if model_name != 'prompt':
                display_name = metrics['display_name']
                row[f'{display_name}_output'] = metrics['output']
                row[f'{display_name}_bleu'] = metrics['bleu']
                row[f'{display_name}_rouge_l'] = metrics['rouge_l']
                row[f'{display_name}_bertscore'] = metrics['bertscore']
                row[f'{display_name}_distinct_1'] = metrics['distinct_1']
                row[f'{display_name}_distinct_2'] = metrics['distinct_2']
                row[f'{display_name}_repetition_rate'] = metrics['repetition_rate']
                row[f'{display_name}_coherence'] = metrics['coherence']
                row[f'{display_name}_length'] = metrics['length']
        detailed_results.append(row)
    
    detailed_df = pd.DataFrame(detailed_results)
    detailed_df.to_csv('detailed_comparison_results.csv', index=False, encoding='utf-8-sig')
    
    print("\nâœ… è¯„ä¼°å®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   - detailed_comparison_results.csv (è¯¦ç»†ç»“æœ)")
    print("   - model_comparison_results.png (å¯è§†åŒ–å›¾è¡¨)")
    
    # æœ€ç»ˆæ€»ç»“
    print("\nğŸ¯ æœ€ç»ˆæ€»ç»“:")
    model_wins = {}
    for model_name in evaluator.models.keys():
        if model_name in model_dfs:
            model_wins[model_name] = 0
    
    metrics = ['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 'coherence']
    
    for metric in metrics:
        best_score = -1
        best_models = []
        for model_name in evaluator.models.keys():
            if model_name in model_dfs:
                score = avg_scores[f'{model_name}_{metric}']
                if score > best_score:
                    best_score = score
                    best_models = [model_name]
                elif score == best_score:
                    best_models.append(model_name)
        
        for model_name in best_models:
            model_wins[model_name] += 1
    
    print("å„æ¨¡å‹è·èƒœæŒ‡æ ‡æ•°:")
    for model_name, wins in model_wins.items():
        display_name = evaluator.models[model_name][0]
        print(f"  {display_name}: {wins} ä¸ªæŒ‡æ ‡")
    
    # æ‰¾å‡ºæ€»ä½“æœ€ä½³æ¨¡å‹
    best_model = max(model_wins.items(), key=lambda x: x[1])
    best_display_name = evaluator.models[best_model[0]][0]
    print(f"\nğŸ† æ€»ä½“æœ€ä½³æ¨¡å‹: {best_display_name} (åœ¨ {best_model[1]} ä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³)")
    
    if best_model[0] == 'your_model':
        print("ğŸ‰ æ­å–œï¼æ‚¨çš„æ¨¡å‹åœ¨å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ï¼")
    else:
        print(f"âš ï¸ {best_display_name} åœ¨å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°æ›´å¥½ï¼Œæ‚¨çš„æ¨¡å‹ä»æœ‰æ”¹è¿›ç©ºé—´")

if __name__ == "__main__":
    main()
