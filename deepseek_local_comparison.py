import torch
import sentencepiece as spm
import requests
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥OpenAI SDK
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    print("âŒ æœªå®‰è£…OpenAI SDKï¼Œè¯·è¿è¡Œ: pip install openai")
    OPENAI_AVAILABLE = False

# è®¾ç½®è‹±æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'Helvetica']
matplotlib.rcParams['font.size'] = 12

class DeepSeekLocalComparison:
    def __init__(self, deepseek_api_key):
        """
        åˆå§‹åŒ–æ¯”è¾ƒå™¨
        
        Args:
            deepseek_api_key: DeepSeek APIå¯†é’¥
        """
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½tokenizer
        self.sp = spm.SentencePieceProcessor()
        self.sp.load("tokenizer.model")
        self.vocab_size = self.sp.get_piece_size()
        print(f"è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        
        # æ¨¡å‹å‚æ•°
        self.d_model = 512
        self.max_seq_len = 2048
        self.h = 8
        self.Nx = 6
        self.dropout_rate = 0.2
        
        # åŠ è½½æ‚¨çš„æœ¬åœ°æ¨¡å‹
        self.your_model = self.load_your_local_model()
        
        self.deepseek_api_key = deepseek_api_key
        
        # æµ‹è¯•æç¤º
        self.test_prompts = [
            "å…³é”®è¯: ä¿¡ å¤©æ¶¯ æ™šé£",
            "å…³é”®è¯: é£ é›¾ å¯‚å¯", 
            "å…³é”®è¯: è´´å¿ƒ æ”¹å˜ è‡ªä¿¡",
            "å…³é”®è¯: åˆå¤œ å¯’å†¬ å¿ƒåŠ¨",
            "å…³é”®è¯: æ€è€ƒ æ¨ç† åˆ†æ",
            "å…³é”®è¯: æœˆå…‰ æ€å¿µ è¿œæ–¹",
            "å…³é”®è¯: æ¢¦æƒ³ åšæŒ æˆåŠŸ",
            "å…³é”®è¯: æ˜¥å¤© å¸Œæœ› æ–°ç”Ÿ",
            "å…³é”®è¯: å­¦ä¹  è¿›æ­¥ æˆé•¿",
            "å…³é”®è¯: å‹è°Š ä¿¡ä»» é™ªä¼´"
        ]
        
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        
        print("ğŸš€ DeepSeekå¯¹æ¯”è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æ‚¨çš„æ¨¡å‹: æœ¬åœ°è®­ç»ƒæ¨¡å‹")
        print(f"DeepSeekæ¨¡å‹: APIè°ƒç”¨")
        print(f"æµ‹è¯•æç¤ºæ•°é‡: {len(self.test_prompts)}")
    
    def load_your_local_model(self):
        """åŠ è½½æ‚¨çš„æœ¬åœ°GPTæ¨¡å‹"""
        try:
            from model_optimized import MemoryOptimizedBigramLM
            
            model = MemoryOptimizedBigramLM(
                vocab_size=self.vocab_size,
                d_model=self.d_model,
                max_seq_len=self.max_seq_len,
                h=self.h,
                Nx=self.Nx,
                dropout_rate=self.dropout_rate
            )
            
            # å°è¯•åŠ è½½æœ€æ–°çš„æ¨¡å‹
            checkpoint_paths = [
                "saved_models/gpt_model_enhanced_stop_20251005_192151.pth"
            ]
            
            loaded = False
            for checkpoint_path in checkpoint_paths:
                try:
                    checkpoint = torch.load(checkpoint_path, 
                                          map_location=self.device, weights_only=False)
                    state_dict = checkpoint['model_state_dict']
                    filtered_state_dict = {k: v for k, v in state_dict.items() if 'mask' not in k}
                    model.load_state_dict(filtered_state_dict, strict=False)
                    print(f"âœ… æˆåŠŸåŠ è½½æ‚¨çš„GPTæ¨¡å‹: {checkpoint_path}")
                    loaded = True
                    break
                except Exception as e:
                    print(f"âŒ åŠ è½½ {checkpoint_path} å¤±è´¥: {e}")
                    continue
            
            if not loaded:
                print("âŒ æ‰€æœ‰æ¨¡å‹æ–‡ä»¶åŠ è½½å¤±è´¥")
                return None
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ‚¨çš„æ¨¡å‹å¤±è´¥: {e}")
            return None
        
        model = model.to(self.device)
        model.eval()
        return model
    
    def generate_with_your_model(self, prompt, max_new_tokens=200):
        """ä½¿ç”¨æ‚¨çš„æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
        if self.your_model is None:
            return ""
        
        temperature = 0.8
        top_k = 50
        repetition_penalty = 1.3
        
        prompt_tokens = self.sp.encode(prompt, out_type=int)
        context = torch.tensor([prompt_tokens], dtype=torch.long, device=self.device)
        
        with torch.no_grad():
            generated_tokens = self.your_model.generate(
                context, 
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_k=top_k,
                repetition_penalty=repetition_penalty
            )[0].tolist()
            
            generated_text = self.sp.decode(generated_tokens)
            response_start = generated_text.find(prompt) + len(prompt)
            response = generated_text[response_start:].strip()
            
            return response
    
    def call_deepseek_api(self, prompt, max_tokens=200):
        """è°ƒç”¨DeepSeek API - ä½¿ç”¨å®˜æ–¹OpenAI SDK"""
        if not OPENAI_AVAILABLE:
            print("âŒ OpenAI SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")
            return ""
            
        try:
            # ä¸ºDeepSeekå¯¹è¯æ¨¡å‹æ·»åŠ æ˜ç¡®çš„æŒ‡ä»¤
            enhanced_prompt = self.enhance_prompt_for_deepseek(prompt)
            
            print(f"ğŸ” æ­£åœ¨è°ƒç”¨DeepSeek API (ä½¿ç”¨OpenAI SDK)...")
            print(f"   åŸå§‹æç¤º: {prompt[:50]}...")
            print(f"   å¢å¼ºæç¤º: {enhanced_prompt[:80]}...")
            
            # æŒ‰ç…§å®˜æ–¹ç¤ºä¾‹åˆ›å»ºå®¢æˆ·ç«¯
            client = OpenAI(
                api_key=self.deepseek_api_key,
                base_url="https://api.deepseek.com"
            )
            
            # è°ƒç”¨API
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯—æ­Œåˆ›ä½œåŠ©æ‰‹ï¼Œæ“…é•¿åˆ›ä½œä¼˜ç¾çš„ä¸­æ–‡è¯—æ­Œã€‚"},
                    {"role": "user", "content": enhanced_prompt},
                ],
                max_tokens=max_tokens,
                temperature=0.8,
                top_p=0.9,
                stream=False
            )
            
            content = response.choices[0].message.content
            print(f"âœ… DeepSeek APIè°ƒç”¨æˆåŠŸï¼Œè¾“å‡ºé•¿åº¦: {len(content)}")
            return content
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨DeepSeek APIæ—¶å‡ºé”™: {e}")
            return ""
    
    def enhance_prompt_for_deepseek(self, prompt):
        """ä¸ºDeepSeekå¯¹è¯æ¨¡å‹å¢å¼ºæç¤ºè¯"""
        # æ ¹æ®æç¤ºç±»å‹æ·»åŠ ä¸åŒçš„æŒ‡ä»¤
        if "å…³é”®è¯:" in prompt:
            # æå–å…³é”®è¯
            keywords = prompt.replace("å…³é”®è¯:", "").strip()
            enhanced_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å…³é”®è¯åˆ›ä½œä¸€é¦–ä¼˜ç¾çš„ä¸­æ–‡è¯—æ­Œï¼š

å…³é”®è¯ï¼š{keywords}

è¦æ±‚ï¼š
1. å¿…é¡»æ˜¯ä¸€é¦–å®Œæ•´çš„è¯—æ­Œ
2. è¯—æ­Œè¦æœ‰æ„å¢ƒå’Œç¾æ„Ÿ
3. åˆç†è¿ç”¨ç»™å®šçš„å…³é”®è¯
4. è¯—æ­Œæ ¼å¼å¯ä»¥æ˜¯ç°ä»£è¯—æˆ–å¤ä½“è¯—
5. ç›´æ¥è¾“å‡ºè¯—æ­Œå†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜

è¯·å¼€å§‹åˆ›ä½œï¼š"""
        elif "è¯·å†™ä¸€é¦–å…³äº" in prompt:
            # æå–ä¸»é¢˜
            theme = prompt.replace("è¯·å†™ä¸€é¦–å…³äº", "").replace("çš„è¯—", "").strip()
            enhanced_prompt = f"""è¯·åˆ›ä½œä¸€é¦–å…³äº{theme}çš„ä¼˜ç¾ä¸­æ–‡è¯—æ­Œã€‚

è¦æ±‚ï¼š
1. å¿…é¡»æ˜¯ä¸€é¦–å®Œæ•´çš„è¯—æ­Œ
2. å›´ç»•{theme}ä¸»é¢˜å±•å¼€
3. è¯—æ­Œè¦æœ‰æ„å¢ƒå’Œç¾æ„Ÿ
4. ç›´æ¥è¾“å‡ºè¯—æ­Œå†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜

è¯·å¼€å§‹åˆ›ä½œï¼š"""
        elif "æè¿°" in prompt or "è§£é‡Š" in prompt or "å†™ä¸€æ®µ" in prompt:
            # è¯´æ˜æ–‡ç±»å‹
            enhanced_prompt = f"""{prompt}

è¯·ç”¨ä¼˜ç¾ã€æµç•…çš„ä¸­æ–‡è¿›è¡Œå›ç­”ï¼Œç›´æ¥ç»™å‡ºå†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜ã€‚"""
        else:
            # å…¶ä»–ç±»å‹æç¤º
            enhanced_prompt = f"""{prompt}

è¯·ç”¨ä¼˜ç¾ã€æµç•…çš„ä¸­æ–‡è¿›è¡Œå›ç­”ï¼Œç›´æ¥ç»™å‡ºå†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜ã€‚"""
        
        return enhanced_prompt
    
    def calculate_bleu_score(self, generated, reference=None):
        """è®¡ç®—BLEUåˆ†æ•°"""
        if reference is None:
            reference = [generated.split()[:5]]
        
        smoothie = SmoothingFunction().method4
        try:
            score = sentence_bleu([reference], generated.split(), smoothing_function=smoothie)
            return score
        except:
            return 0.0
    
    def calculate_rouge_l(self, generated, reference=None):
        """è®¡ç®—ROUGE-Låˆ†æ•°"""
        if reference is None:
            reference = generated[:50]
        
        scores = self.rouge_scorer.score(reference, generated)
        return scores['rougeL'].fmeasure
    
    def calculate_bertscore(self, generated, reference=None):
        """è®¡ç®—BERTScore"""
        if reference is None:
            reference = generated
        
        try:
            P, R, F1 = bert_score([generated], [reference], lang="zh", verbose=False)
            return F1.item()
        except:
            return 0.0
    
    def calculate_distinct_n(self, text, n):
        """è®¡ç®—distinct-næŒ‡æ ‡"""
        words = text.split()
        if len(words) < n:
            return 0.0
        
        ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]
        unique_ngrams = len(set(ngrams))
        total_ngrams = len(ngrams)
        
        return unique_ngrams / total_ngrams if total_ngrams > 0 else 0.0
    
    def calculate_repetition_rate(self, text):
        """è®¡ç®—é‡å¤ç‡"""
        words = text.split()
        if len(words) < 2:
            return 0.0
        
        repeated_count = 0
        total_pairs = len(words) - 1
        
        for i in range(total_pairs):
            if words[i] == words[i+1]:
                repeated_count += 1
        
        return repeated_count / total_pairs if total_pairs > 0 else 0.0
    
    def calculate_coherence_score(self, text):
        """è®¡ç®—è¿è´¯æ€§åˆ†æ•°"""
        sentences = text.split('ã€‚')
        if len(sentences) < 2:
            return 0.5
        
        sentence_lengths = [len(sent) for sent in sentences if len(sent) > 0]
        if len(sentence_lengths) < 2:
            return 0.5
        
        length_std = np.std(sentence_lengths)
        coherence = 1.0 - min(length_std / 20, 1.0)
        
        return coherence
    
    def evaluate_single_prompt(self, prompt):
        """è¯„ä¼°å•ä¸ªæç¤ºçš„ä¸¤ä¸ªæ¨¡å‹è¾“å‡º"""
        print(f"å¤„ç†æç¤º: {prompt}")
        
        # è°ƒç”¨ä¸¤ä¸ªæ¨¡å‹
        your_model_output = self.generate_with_your_model(prompt)
        time.sleep(1)  # é¿å…APIé™æµ
        deepseek_output = self.call_deepseek_api(prompt)
        
        print(f"æ‚¨çš„æ¨¡å‹è¾“å‡ºé•¿åº¦: {len(your_model_output)}")
        print(f"DeepSeekè¾“å‡ºé•¿åº¦: {len(deepseek_output)}")
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'your_model': {
                'output': your_model_output,
                'bleu': self.calculate_bleu_score(your_model_output),
                'rouge_l': self.calculate_rouge_l(your_model_output),
                'bertscore': self.calculate_bertscore(your_model_output),
                'distinct_1': self.calculate_distinct_n(your_model_output, 1),
                'distinct_2': self.calculate_distinct_n(your_model_output, 2),
                'repetition_rate': self.calculate_repetition_rate(your_model_output),
                'coherence': self.calculate_coherence_score(your_model_output),
                'length': len(your_model_output)
            },
            'deepseek': {
                'output': deepseek_output,
                'bleu': self.calculate_bleu_score(deepseek_output),
                'rouge_l': self.calculate_rouge_l(deepseek_output),
                'bertscore': self.calculate_bertscore(deepseek_output),
                'distinct_1': self.calculate_distinct_n(deepseek_output, 1),
                'distinct_2': self.calculate_distinct_n(deepseek_output, 2),
                'repetition_rate': self.calculate_repetition_rate(deepseek_output),
                'coherence': self.calculate_coherence_score(deepseek_output),
                'length': len(deepseek_output)
            }
        }
        
        return metrics
    
    def run_comparison(self):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”è¯„ä¼°"""
        print("å¼€å§‹DeepSeekå¯¹æ¯”è¯„ä¼°...")
        print("=" * 80)
        
        all_results = []
        
        for i, prompt in enumerate(self.test_prompts, 1):
            print(f"\nè¿›åº¦: {i}/{len(self.test_prompts)}")
            result = self.evaluate_single_prompt(prompt)
            result['prompt'] = prompt
            all_results.append(result)
            
            # æ¯3ä¸ªæç¤ºåä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…APIé™æµ
            if i % 3 == 0:
                print("ä¼‘æ¯5ç§’...")
                time.sleep(5)
        
        return all_results
    
    def analyze_results(self, all_results):
        """åˆ†æå¹¶å¯è§†åŒ–ç»“æœ"""
        # æå–æ•°æ®
        your_model_scores = []
        deepseek_scores = []
        
        for result in all_results:
            your_model_scores.append(result['your_model'])
            deepseek_scores.append(result['deepseek'])
        
        # åˆ›å»ºDataFrame
        your_model_df = pd.DataFrame(your_model_scores)
        deepseek_df = pd.DataFrame(deepseek_scores)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        metrics = ['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 
                  'repetition_rate', 'coherence', 'length']
        
        your_model_avg = {metric: your_model_df[metric].mean() for metric in metrics}
        deepseek_avg = {metric: deepseek_df[metric].mean() for metric in metrics}
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "="*80)
        print("DeepSeekå¯¹æ¯”è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*80)
        
        for metric in metrics:
            print(f"\n{metric.upper():<15}:")
            print(f"  æ‚¨çš„æ¨¡å‹: {your_model_avg[metric]:.4f}")
            print(f"  DeepSeek: {deepseek_avg[metric]:.4f}")
            
            if your_model_avg[metric] > deepseek_avg[metric]:
                print(f"  ğŸ‰ æ‚¨çš„æ¨¡å‹é¢†å…ˆ: +{your_model_avg[metric] - deepseek_avg[metric]:.4f}")
            elif deepseek_avg[metric] > your_model_avg[metric]:
                print(f"  âš ï¸ DeepSeeké¢†å…ˆ: +{deepseek_avg[metric] - your_model_avg[metric]:.4f}")
            else:
                print(f"  ğŸ¤ å¹³å±€")
        
        return your_model_df, deepseek_df, your_model_avg, deepseek_avg
    
    def create_visualizations(self, your_model_df, deepseek_df, your_model_avg, deepseek_avg):
        """Create visualization charts"""
        # Set chart style
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('DeepSeek vs Your Local Model - Performance Comparison Analysis', fontsize=16, fontweight='bold')
        
        # Color settings
        colors = ['#3498db', '#e74c3c']  # Blue - Your model, Red - DeepSeek
        
        # 1. Main metrics comparison - Bar chart
        ax1 = axes[0, 0]
        metrics_to_plot = ['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 'coherence']
        metric_names = ['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Distinct-2', 'Coherence']
        
        x = np.arange(len(metrics_to_plot))
        width = 0.35
        
        your_model_values = [your_model_avg[metric] for metric in metrics_to_plot]
        deepseek_values = [deepseek_avg[metric] for metric in metrics_to_plot]
        
        ax1.bar(x - width/2, your_model_values, width, label='Your Model', color=colors[0], alpha=0.8)
        ax1.bar(x + width/2, deepseek_values, width, label='DeepSeek', color=colors[1], alpha=0.8)
        
        ax1.set_xlabel('Evaluation Metrics', fontsize=12)
        ax1.set_ylabel('Score', fontsize=12)
        ax1.set_title('Main Metrics Comparison', fontsize=14, fontweight='bold')
        ax1.set_xticks(x)
        ax1.set_xticklabels(metric_names, rotation=45, fontsize=10)
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # 2. Repetition rate comparison - Box plot
        ax2 = axes[0, 1]
        repetition_data = [your_model_df['repetition_rate'], deepseek_df['repetition_rate']]
        box_plot = ax2.boxplot(repetition_data, labels=['Your Model', 'DeepSeek'], patch_artist=True)
        
        for i, patch in enumerate(box_plot['boxes']):
            patch.set_facecolor(colors[i])
        
        ax2.set_ylabel('Repetition Rate', fontsize=12)
        ax2.set_title('Repetition Rate Distribution', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # 3. Output length comparison - Box plot
        ax3 = axes[0, 2]
        length_data = [your_model_df['length'], deepseek_df['length']]
        length_plot = ax3.boxplot(length_data, labels=['Your Model', 'DeepSeek'], patch_artist=True)
        
        for i, patch in enumerate(length_plot['boxes']):
            patch.set_facecolor(colors[i])
        
        ax3.set_ylabel('Output Length (characters)', fontsize=12)
        ax3.set_title('Output Length Comparison', fontsize=14, fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # 4. Performance radar chart
        ax4 = axes[1, 0]
        radar_metrics = ['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'coherence']
        radar_names = ['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Coherence']
        
        angles = np.linspace(0, 2*np.pi, len(radar_metrics), endpoint=False).tolist()
        angles += angles[:1]
        
        your_model_radar = [your_model_avg[metric] for metric in radar_metrics]
        deepseek_radar = [deepseek_avg[metric] for metric in radar_metrics]
        
        # Normalization
        max_vals = [max(your_model_radar[i], deepseek_radar[i]) for i in range(len(radar_metrics))]
        your_model_radar_norm = [your_model_radar[i] / max_vals[i] if max_vals[i] > 0 else 0 
                               for i in range(len(radar_metrics))]
        deepseek_radar_norm = [deepseek_radar[i] / max_vals[i] if max_vals[i] > 0 else 0 
                             for i in range(len(radar_metrics))]
        
        your_model_radar_norm += your_model_radar_norm[:1]
        deepseek_radar_norm += deepseek_radar_norm[:1]
        
        ax4.plot(angles, your_model_radar_norm, 'o-', linewidth=2, label='Your Model', color=colors[0])
        ax4.fill(angles, your_model_radar_norm, alpha=0.25, color=colors[0])
        ax4.plot(angles, deepseek_radar_norm, 'o-', linewidth=2, label='DeepSeek', color=colors[1])
        ax4.fill(angles, deepseek_radar_norm, alpha=0.25, color=colors[1])
        
        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(radar_names, fontsize=10)
        ax4.set_ylim(0, 1)
        ax4.set_title('Performance Radar Chart', fontsize=14, fontweight='bold')
        ax4.legend(fontsize=10)
        ax4.grid(True, alpha=0.3)
        
        # 5. Metric correlation heatmap - Your model
        ax5 = axes[1, 1]
        correlation_matrix_your = your_model_df[['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 'coherence']].corr()
        sns.heatmap(correlation_matrix_your, annot=True, cmap='coolwarm', center=0, ax=ax5, 
                   xticklabels=['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Distinct-2', 'Coherence'],
                   yticklabels=['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Distinct-2', 'Coherence'],
                   annot_kws={"size": 9})
        ax5.set_title('Your Model: Metric Correlation', fontsize=14, fontweight='bold')
        
        # 6. Metric correlation heatmap - DeepSeek
        ax6 = axes[1, 2]
        correlation_matrix_deepseek = deepseek_df[['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 'coherence']].corr()
        sns.heatmap(correlation_matrix_deepseek, annot=True, cmap='coolwarm', center=0, ax=ax6,
                   xticklabels=['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Distinct-2', 'Coherence'],
                   yticklabels=['BLEU', 'ROUGE-L', 'BERTScore', 'Distinct-1', 'Distinct-2', 'Coherence'],
                   annot_kws={"size": 9})
        ax6.set_title('DeepSeek: Metric Correlation', fontsize=14, fontweight='bold')
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig('deepseek_local_comparison_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return fig
    
    def save_detailed_results(self, all_results):
        """ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSV"""
        detailed_results = []
        for result in all_results:
            row = {'prompt': result['prompt']}
            row['your_model_output'] = result['your_model']['output']
            row['deepseek_output'] = result['deepseek']['output']
            
            for metric in ['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 'repetition_rate', 'coherence', 'length']:
                row[f'your_model_{metric}'] = result['your_model'][metric]
                row[f'deepseek_{metric}'] = result['deepseek'][metric]
            
            detailed_results.append(row)
        
        detailed_df = pd.DataFrame(detailed_results)
        detailed_df.to_csv('deepseek_local_comparison_results.csv', index=False, encoding='utf-8-sig')
        print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: deepseek_local_comparison_results.csv")
        
        return detailed_df

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡ŒDeepSeekå¯¹æ¯”è¯„ä¼°"""
    print("ğŸš€ DeepSeekå¯¹æ¯”è¯„ä¼° - æœ¬åœ°æ¨¡å‹ vs DeepSeek API")
    print("=" * 80)
    
    
    # é…ç½®DeepSeek APIå¯†é’¥
    DEEPSEEK_API_KEY = "sk-e1c7fb08748f4f4fa642065595069962"  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„DeepSeek APIå¯†é’¥
    
    # åˆ›å»ºæ¯”è¾ƒå™¨
    if DEEPSEEK_API_KEY == "YOUR_DEEPSEEK_API_KEY_HERE":
        print("âŒ è¯·å…ˆé…ç½®æ‚¨çš„DeepSeek APIå¯†é’¥")
        print("è¯·ç¼–è¾‘ deepseek_local_comparison.py æ–‡ä»¶ï¼Œå°† YOUR_DEEPSEEK_API_KEY_HERE æ›¿æ¢ä¸ºæ‚¨çš„APIå¯†é’¥")
        return
    
    comparator = DeepSeekLocalComparison(deepseek_api_key=DEEPSEEK_API_KEY)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½æˆåŠŸ
    if comparator.your_model is None:
        print("âŒ æ‚¨çš„æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”")
        return
    
    # è¿è¡Œå¯¹æ¯”è¯„ä¼°
    all_results = comparator.run_comparison()
    
    # åˆ†æç»“æœ
    your_model_df, deepseek_df, your_model_avg, deepseek_avg = comparator.analyze_results(all_results)
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    comparator.create_visualizations(your_model_df, deepseek_df, your_model_avg, deepseek_avg)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    comparator.save_detailed_results(all_results)
    
    # æœ€ç»ˆæ€»ç»“
    print("\nğŸ¯ æœ€ç»ˆæ€»ç»“:")
    metrics = ['bleu', 'rouge_l', 'bertscore', 'distinct_1', 'distinct_2', 'coherence']
    
    your_model_wins = 0
    deepseek_wins = 0
    ties = 0
    
    for metric in metrics:
        your_score = your_model_avg[metric]
        deepseek_score = deepseek_avg[metric]
        
        if your_score > deepseek_score:
            your_model_wins += 1
        elif deepseek_score > your_score:
            deepseek_wins += 1
        else:
            ties += 1
    
    print(f"æ‚¨çš„æ¨¡å‹è·èƒœæŒ‡æ ‡æ•°: {your_model_wins}")
    print(f"DeepSeekè·èƒœæŒ‡æ ‡æ•°: {deepseek_wins}")
    print(f"å¹³å±€æŒ‡æ ‡æ•°: {ties}")
    
    if your_model_wins > deepseek_wins:
        print(f"\nğŸ† æ€»ä½“æœ€ä½³æ¨¡å‹: æ‚¨çš„æœ¬åœ°æ¨¡å‹ (åœ¨ {your_model_wins} ä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³)")
        print("ğŸ‰ æ­å–œï¼æ‚¨çš„æœ¬åœ°æ¨¡å‹åœ¨å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºDeepSeekï¼")
    elif deepseek_wins > your_model_wins:
        print(f"\nğŸ† æ€»ä½“æœ€ä½³æ¨¡å‹: DeepSeek (åœ¨ {deepseek_wins} ä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³)")
        print("âš ï¸ DeepSeekåœ¨å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°æ›´å¥½ï¼Œæ‚¨çš„æ¨¡å‹ä»æœ‰æ”¹è¿›ç©ºé—´")
    else:
        print(f"\nğŸ¤ æ€»ä½“å¹³å±€ (åŒæ–¹å„åœ¨ {your_model_wins} ä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³)")
        print("æ‚¨çš„æœ¬åœ°æ¨¡å‹ä¸DeepSeekè¡¨ç°ç›¸å½“ï¼")
    
    print("\nâœ… è¯„ä¼°å®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   - deepseek_local_comparison_results.csv (è¯¦ç»†ç»“æœ)")
    print("   - deepseek_local_comparison_results.png (å¯è§†åŒ–å›¾è¡¨)")

if __name__ == "__main__":
    main()
