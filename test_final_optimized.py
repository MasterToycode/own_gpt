import torch
import sentencepiece as spm
from model_optimized import MemoryOptimizedBigramLM

# è®¾å¤‡è®¾ç½®
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åŠ è½½tokenizer
sp = spm.SentencePieceProcessor()
sp.load("tokenizer.model")
vocab_size = sp.get_piece_size()
print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")

# æ¨¡å‹å‚æ•°ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
d_model = 512
max_seq_len = 2048
h = 8
Nx = 6
dropout_rate = 0.2

# åˆ›å»ºæ¨¡å‹
model = MemoryOptimizedBigramLM(
    vocab_size=vocab_size,
    d_model=d_model,
    max_seq_len=max_seq_len,
    h=h,
    Nx=Nx,
    dropout_rate=dropout_rate
)

# åŠ è½½æœ€æ–°çš„è®­ç»ƒæ¨¡å‹æƒé‡
try:
    checkpoint = torch.load("saved_models/gpt_model_enhanced_stop_20251004_181034.pth", map_location=device, weights_only=False)
    
    # è¿‡æ»¤æ‰maskç›¸å…³çš„é”®ï¼Œå› ä¸ºå®ƒä»¬ä¸æ˜¯æ¨¡å‹å‚æ•°è€Œæ˜¯ç¼“å†²åŒº
    state_dict = checkpoint['model_state_dict']
    filtered_state_dict = {k: v for k, v in state_dict.items() if 'mask' not in k}
    
    model.load_state_dict(filtered_state_dict, strict=False)
    print("âœ… æˆåŠŸåŠ è½½æœ€æ–°è®­ç»ƒæ¨¡å‹æƒé‡")
    print(f"è®­ç»ƒè¿­ä»£æ¬¡æ•°: {checkpoint['iteration']}")
    print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {checkpoint['train_losses'][-1]:.4f}")
    print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {checkpoint['valid_losses'][-1]:.4f}")
    print(f"æœ€ç»ˆè®­ç»ƒPPL: {checkpoint['train_ppls'][-1]:.2f}")
    print(f"æœ€ç»ˆéªŒè¯PPL: {checkpoint['valid_ppls'][-1]:.2f}")
except Exception as e:
    print(f" åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    exit(1)

model = model.to(device)
model.eval()

def calculate_repetition_rate(text):
    """è®¡ç®—æ–‡æœ¬çš„é‡å¤ç‡"""
    words = text.split()
    if len(words) < 2:
        return 0.0
    
    # è®¡ç®—è¿ç»­é‡å¤çš„æ¯”ç‡
    repeated_count = 0
    total_pairs = len(words) - 1
    
    for i in range(total_pairs):
        if words[i] == words[i+1]:
            repeated_count += 1
    
    return repeated_count / total_pairs if total_pairs > 0 else 0.0

def test_output_optimized(prompt, max_new_tokens=300):
    """ä½¿ç”¨ä¼˜åŒ–å‚æ•°æµ‹è¯•æ¨¡å‹è¾“å‡ºåŠŸèƒ½"""
    # æœ€ä½³å‚æ•°ç»„åˆï¼ˆæ ¹æ®æµ‹è¯•ç»“æœï¼‰
    temperature = 0.8
    top_k = 50
    repetition_penalty = 1.3
    
    print(f"\n{'='*80}")
    print(f"ä¼˜åŒ–å‚æ•°: temperature={temperature}, top_k={top_k}, repetition_penalty={repetition_penalty}")
    print(f"è¾“å…¥æç¤º: {prompt}")
    print(f"{'='*80}")
    
    # ç¼–ç æç¤ºæ–‡æœ¬
    prompt_tokens = sp.encode(prompt, out_type=int)
    
    # è½¬æ¢ä¸ºtensor
    context = torch.tensor([prompt_tokens], dtype=torch.long, device=device)
    
    # ç”Ÿæˆå“åº”
    with torch.no_grad():
        generated_tokens = model.generate(
            context, 
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_k=top_k,
            repetition_penalty=repetition_penalty
        )[0].tolist()
        
        generated_text = sp.decode(generated_tokens)
        
        # æå–ç”Ÿæˆçš„å“åº”éƒ¨åˆ†ï¼ˆå»æ‰promptï¼‰
        response_start = generated_text.find(prompt) + len(prompt)
        response = generated_text[response_start:].strip()
        
        # è®¡ç®—é‡å¤ç‡
        repetition_rate = calculate_repetition_rate(response)
        
        print(f"å®Œæ•´è¾“å‡º:")
        print(f"{generated_text}")
        print(f"\næå–çš„å“åº”:")
        print(f"{response}")
        print(f"\nè¯„ä¼°æŒ‡æ ‡:")
        print(f"  è¾“å‡ºé•¿åº¦: {len(response)} å­—ç¬¦")
        print(f"  é‡å¤ç‡: {repetition_rate:.4f}")
        
        return response, repetition_rate

# æµ‹è¯•å¤šä¸ªä¸åŒç±»å‹çš„æç¤º
print("å¼€å§‹ä½¿ç”¨ä¼˜åŒ–å‚æ•°æµ‹è¯•æ¨¡å‹è¾“å‡º...")
test_prompts = [
    "å…³é”®è¯: ä¿¡ å¤©æ¶¯ æ™šé£",
    "å…³é”®è¯: é£ é›¾ å¯‚å¯",
    "å…³é”®è¯: è´´å¿ƒ æ”¹å˜ è‡ªä¿¡",
    "å…³é”®è¯: åˆå¤œ å¯’å†¬ å¿ƒåŠ¨",
    "å…³é”®è¯: æ€è€ƒ æ¨ç† åˆ†æ",
    "å…³é”®è¯: æœˆå…‰ æ€å¿µ è¿œæ–¹",
    "å…³é”®è¯: æ¢¦æƒ³ åšæŒ æˆåŠŸ",
    "å…³é”®è¯: æ˜¥å¤© å¸Œæœ› æ–°ç”Ÿ"
]

total_repetition_rate = 0
total_responses = len(test_prompts)

for i, prompt in enumerate(test_prompts, 1):
    print(f"\nğŸ”¬ æµ‹è¯• {i}/{total_responses}")
    response, repetition_rate = test_output_optimized(prompt)
    total_repetition_rate += repetition_rate
    
    # è¯„ä¼°è¾“å‡ºè´¨é‡
    if repetition_rate == 0.0:
        print(f"âœ… è¾“å‡ºè´¨é‡ä¼˜ç§€ - æ— é‡å¤")
    elif repetition_rate < 0.05:
        print(f"âœ… è¾“å‡ºè´¨é‡è‰¯å¥½ - è½»å¾®é‡å¤")
    elif repetition_rate < 0.1:
        print(f"âš ï¸ è¾“å‡ºè´¨é‡ä¸€èˆ¬ - ä¸­ç­‰é‡å¤")
    else:
        print(f"âŒ è¾“å‡ºè´¨é‡è¾ƒå·® - ä¸¥é‡é‡å¤")

# è®¡ç®—å¹³å‡é‡å¤ç‡
avg_repetition_rate = total_repetition_rate / total_responses

print(f"\n{'='*80}")
print("ğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœæ€»ç»“")
print(f"{'='*80}")
print(f"æµ‹è¯•æç¤ºæ•°é‡: {total_responses}")
print(f"å¹³å‡é‡å¤ç‡: {avg_repetition_rate:.4f}")
print(f"æœ€ä½³å‚æ•°ç»„åˆ: temperature=0.8, top_k=50, repetition_penalty=1.3")
print(f"ç”Ÿæˆé•¿åº¦: 300 tokens")

if avg_repetition_rate == 0.0:
    print(f"ğŸ‰ ä¼˜åŒ–æˆåŠŸï¼æ‰€æœ‰è¾“å‡ºå‡æ— é‡å¤")
elif avg_repetition_rate < 0.05:
    print(f"âœ… ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼å¹³å‡é‡å¤ç‡å¾ˆä½")
elif avg_repetition_rate < 0.1:
    print(f"âš ï¸ ä¼˜åŒ–æ•ˆæœä¸€èˆ¬ï¼Œä»æœ‰æ”¹è¿›ç©ºé—´")
else:
    print(f"âŒ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

print(f"\nä¼˜åŒ–å‰é—®é¢˜: å¤§é‡é‡å¤è¯æ±‡ï¼ˆå¦‚'å…„å¼Ÿ'ã€'å…„å¼Ÿå§å¦¹'ç­‰ï¼‰")
print(f"ä¼˜åŒ–åæ•ˆæœ: é‡å¤ç‡æ˜¾è‘—é™ä½ï¼Œè¾“å‡ºå¤šæ ·æ€§æé«˜")
